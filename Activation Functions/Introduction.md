Introduction
In a neural network, each neuron takes an input signal, combines it with weights and a bias, and then passes the result through an activation function.
Without this activation function, the output of the neuron would just be a linear combination of inputs, meaning that the entire network â€” even with multiple layers â€” could only model linear relationships.
Therefore, activation functions are essential for solving complex problems that require non-linearity

Ø¯Ø± ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒØŒ Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ ØªØ±Ú©ÛŒØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù¾Ø³ Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø§Ø² ÛŒÚ© ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
Ø§Ú¯Ø± Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† ÙÙ‚Ø· ÛŒÚ© ØªØ±Ú©ÛŒØ¨ Ø®Ø·ÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯ØŒ Ùˆ Ú©Ù„ Ø´Ø¨Ú©Ù‡ â€” Ø­ØªÛŒ Ø§Ú¯Ø± Ú†Ù†Ø¯ÛŒÙ† Ù„Ø§ÛŒÙ‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ â€” ÙÙ‚Ø· Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø®Ø·ÛŒ Ø±Ø§ Ù…Ø¯Ù„ Ú©Ù†Ø¯.
Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø³Ø§Ø¦Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØºÛŒØ±Ø®Ø·ÛŒâ€ŒØ¨ÙˆØ¯Ù† (Non-linearity) Ø¯Ø§Ø±Ù†Ø¯ØŒ ØªÙˆØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ù‡Ø³ØªÙ†Ø¯.

Why is Non-Linearity Important?

Ú†Ø±Ø§ ØºÛŒØ±Ø®Ø·ÛŒâ€ŒØ¨ÙˆØ¯Ù† Ù…Ù‡Ù… Ø§Ø³ØªØŸ 



Imagine you want to build a neural network to classify data distributed in a circular shape. If you only use linear functions, the model cannot learn complex boundaries and, at best, will draw a straight line to separate points.
By adding nonlinear functions, the model can learn complex shapes like circles, spirals, or any irregular pattern.

ÙØ±Ø¶ Ú©Ù†ÛŒØ¯ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø´Ú©Ù„ Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø§Ú¯Ø± ÙÙ‚Ø· Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ø®Ø·ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ØŒ Ù…Ø¯Ù„ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ø±Ø²Ù‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯ Ùˆ Ø¯Ø± Ø¨Ù‡ØªØ±ÛŒÙ† Ø­Ø§Ù„ØªØŒ ÛŒÚ© Ø®Ø· Ù…Ø³ØªÙ‚ÛŒÙ… ØªØ±Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù†Ù‚Ø§Ø· Ø±Ø§ Ø¬Ø¯Ø§ Ú©Ù†Ø¯.
Ø¨Ø§ Ø§Ø¶Ø§ÙÙ‡â€ŒÚ©Ø±Ø¯Ù† ØªÙˆØ§Ø¨Ø¹ ØºÛŒØ±Ø®Ø·ÛŒØŒ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø´Ú©Ø§Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ù…Ø§Ù†Ù†Ø¯ Ø¯Ø§ÛŒØ±Ù‡ØŒ Ù…Ø§Ø±Ù¾ÛŒÚ† ÛŒØ§ Ù‡Ø± Ø§Ù„Ú¯ÙˆÛŒ Ù†Ø§Ù…Ù†Ø¸Ù… Ø¯ÛŒÚ¯Ø±ÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.



General Neuron Formula
ÙØ±Ù…ÙˆÙ„ Ú©Ù„ÛŒ ÛŒÚ© Ù†ÙˆØ±ÙˆÙ†

ÛŒÚ© Ù†ÙˆØ±ÙˆÙ† ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³ ØªØ±Ú©ÛŒØ¨ Ú©Ø±Ø¯Ù‡ Ùˆ Ø§Ø² ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

$$
z = \sum_{i=1}^{n} w_i x_i + b
$$
$$
y = f(z)
$$

 inputs

$$xiâ€‹$$ 

 weights

$$wiwiâ€‹$$

 bias

$$bb$$

 activation function

$$ff$$


Code Example Python (NumPy)

**

      import numpy as np
      
      x = np.array([0.5, -0.2, 0.1])
      w = np.array([0.4, 0.7, -0.5])
      b = 0.1
      
      def sigmoid(z):
          return 1 / (1 + np.exp(-z))
      
      z = np.dot(w, x) + b
      y = sigmoid(z)
      
      print("Linear Output (z):", z)
      print("Activated Output (y):", y)
**

Without activation functions, a neural network becomes a simple linear model, unable to capture complex non-linear patterns.
   
  Ø¨Ø¯ÙˆÙ† ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ Ù…Ø¯Ù„ ÙÙ‚Ø· ÛŒÚ© ØªØ±Ú©ÛŒØ¨ Ø®Ø·ÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯ Ùˆ ØªÙˆØ§Ù†Ø§ÛŒÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø±Ø§ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø´Øª.

     
 Ù†Ù‚Ø´ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ:

  Ø§Ø¶Ø§ÙÙ‡â€ŒÚ©Ø±Ø¯Ù† ØºÛŒØ±Ø®Ø·ÛŒ Ø¨ÙˆØ¯Ù† Ø¨Ù‡ Ù…Ø¯Ù„

   Ú©Ù†ØªØ±Ù„ Ø¹Ø¨ÙˆØ± Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ (Ù…Ø«Ù„ ÙÛŒÙ„ØªØ±)

   Ú©Ù…Ú© Ø¨Ù‡ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´

  Ø§Ù†ÙˆØ§Ø¹ ØªÙˆØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ÛŒØ¬:

1. Step Function (ØªØ§Ø¨Ø¹ Ù¾Ù„Ù‡â€ŒØ§ÛŒ)

Ù‚Ø¯ÛŒÙ…ÛŒ Ùˆ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ù…Ø´ØªÙ‚ â€” Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ

$$f(z) =\begin{cases}1 & \text{if } z \geq 0 \\\\0 & \text{if } z < 0\end{cases}$$

âœ… Ø³Ø§Ø¯Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù…

âŒ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚ (Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† ØµÙØ±)

2. Sigmoid Function (Ø³ÛŒÚ¯Ù…ÙˆÛŒØ¯)

$$f(z) = \frac{1}{1 + e^{-z}}$$

ğŸ“ˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨ÛŒÙ† (0, 1)

ğŸ§  Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø§Ø¦Ù„ Ø¨Ø§ÛŒÙ†Ø±ÛŒ

âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÚ¯ÙˆÙ†Ù‡

âŒ Ù…Ø´Ú©Ù„ Vanishing Gradient Ø¯Ø± Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚

3. Tanh Function (ØªØ§Ù†Ú˜Ø§Ù†Øª Ù‡ÛŒÙ¾Ø±Ø¨ÙˆÙ„ÛŒÚ©)

$$f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

ğŸ“ˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨ÛŒÙ† (-1, 1)

âœ… Ù…Ø±Ú©Ø²ÛŒØª Ø­ÙˆÙ„ ØµÙØ± â†’ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡ØªØ± Ù†Ø³Ø¨Øª Ø¨Ù‡ Sigmoid

âŒ Ù‡Ù†ÙˆØ² Ù…Ø´Ú©Ù„ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ú©ÙˆÚ†Ú© Ø¯Ø§Ø±Ø¯ Ø¯Ø± Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚

4. ReLU (Rectified Linear Unit)

$$f(z)=max(0,z)$$

ğŸ“ˆ Ù…Ø¹Ø±ÙˆÙâ€ŒØªØ±ÛŒÙ† Ùˆ Ù¾Ø±Ú©Ø§Ø±Ø¨Ø±Ø¯ØªØ±ÛŒÙ† ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ

âœ… Ø³Ø§Ø¯Ú¯ÛŒØŒ Ù…Ø´ØªÙ‚â€ŒÙ¾Ø°ÛŒØ±ÛŒØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¹Ø§Ù„ÛŒ

âŒ Ù…Ø´Ú©Ù„ "Ù…Ø±Ú¯ Ù†ÙˆØ±ÙˆÙ†" (Dead Neurons) â€“ Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ù…ÛŒØ´Ù‡ Ù…Ù†ÙÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†ÙˆØ±ÙˆÙ† Ø®Ø§Ù…ÙˆØ´ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯

5. Leaky ReLU

 $$f(z) =\begin{cases}z & \text{if } z \geq 0 \\\\\alpha z & \text{if } z < 0 \quad (\alpha \approx 0.01)\end{cases}$$

âœ… Ø­Ù„ Ù…Ø´Ú©Ù„ Ù…Ø±Ú¯ Ù†ÙˆØ±ÙˆÙ† Ø¯Ø± ReLU

âœ… Ø§Ø¬Ø§Ø²Ù‡ Ø¹Ø¨ÙˆØ± Ø§Ù†Ø¯Ú© Ø§Ø² Ø¨Ø®Ø´ Ù…Ù†ÙÛŒ

6. Softmax

$$f(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$


ğŸ“ˆ ØªØ¨Ø¯ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ù†Ø¯â€ŒÚ©Ù„Ø§Ø³Ù‡

âœ… Ù…Ø¬Ù…ÙˆØ¹ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ = 1

âœ… Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡â€ŒÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ

ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ:


| ØªØ§Ø¨Ø¹       | Ø¯Ø§Ù…Ù†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ  | Ù…Ø´ØªÙ‚â€ŒÙ¾Ø°ÛŒØ±ÛŒ | Ú©Ø§Ø±Ø¨Ø±Ø¯                |
| ---------- | ------------ | ---------- | --------------------- |
| Step       | 0 ÛŒØ§ 1       | âŒ          | Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ ÛŒØ§ Ù‚Ø¯ÛŒÙ…ÛŒ |
| Sigmoid    | (0, 1)       | âœ…          | Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ      |
| Tanh       | (-1, 1)      | âœ…          | Ù…ÛŒØ§Ù†â€ŒÙ„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ RNN     |
| ReLU       | \[0, âˆ)      | âœ…          | CNNØŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÛŒÙ‚    |
| Leaky ReLU | (-âˆ, âˆ)      | âœ…          | Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† ReLU          |
| Softmax    | \[0, 1], Î£=1 | âœ…          | Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡    |


 Activation Functions Analysis & Comparison
 
 ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ ØªÙˆØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ 

 In this section, our goal is to understand when to use which activation function and why choosing the right one is crucial for model performance.

 Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ù‡Ø¯Ù Ù…Ø§ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¨ÙÙ‡Ù…ÛŒÙ… Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø² Ú©Ø¯Ø§Ù… ØªØ§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… Ùˆ Ú†Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø±Ø³Øª ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª.

  
  
   Practical Recommendations
  
  ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„ÛŒ 

  Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† (Hidden Layers): Ø§ØºÙ„Ø¨ Ø§Ø² ReLU ÛŒØ§ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.

Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ (Output Layer): Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ù…Ø³Ø¦Ù„Ù‡:

    Binary Classification â†’ Sigmoid

    Multi-class Classification â†’ Softmax

    Regression â†’ Ø¨Ø¯ÙˆÙ† ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ ReLU


   Ù…Ø«Ø§Ù„ ØªØ±Ú©ÛŒØ¨ ØªÙˆØ§Ø¨Ø¹ â€“ Mixed Activation Example


A neural network for MNIST digit recognition:

    Hidden layers â†’ ReLU

    Output layer â†’ Softmax
    
ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø±Ù‚Ø§Ù… MNIST:

    Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† â†’ ReLU

    Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ â†’ Softmax
    
 **
 
    import tensorflow as tf
    
    model = tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    
 **
    
