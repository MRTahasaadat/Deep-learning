1. مقدمه‌ای بر نورون بیولوژیکی (Biological Inspiration)
 
 
 الهام از مغز انسان

   Artificial Neural Networks are inspired by the structure and function of the human brain.
     
   شبکه‌های عصبی مصنوعی از ساختار مغز انسان الهام گرفته شده‌اند.


نورون بیولوژیکی چگونه کار می‌کند؟
 

یک نورون طبیعی در مغز دارای اجزای زیر است:

| جزء                        | توضیح                                             |
| -------------------------- | ------------------------------------------------- |
| **Dendrites (دندریت‌ها)**  | دریافت سیگنال از نورون‌های دیگر                   |
| **Cell Body (سومای سلول)** | پردازش سیگنال ورودی                               |
| **Axon (آکسون)**           | انتقال خروجی به نورون دیگر                        |
| **Synapse (سیناپس)**       | اتصال میان نورون‌ها، که وزن سیگنال را مشخص می‌کند |

 سیگنال‌های ورودی از دندریت‌ها جمع می‌شوند، اگر به سطح آستانه‌ای برسند، سیگنال به‌وسیله‌ی آکسون به نورون بعدی منتقل می‌شود.
 الهام‌گیری برای مدل‌سازی مصنوعی

   هر نورون مصنوعی مانند یک واحد ساده محاسباتی عمل می‌کند، که ورودی‌ها را دریافت کرده، وزن می‌دهد، جمع می‌کند، از روی آن یک تابع عبور می‌دهد، و در نهایت خروجی تولید می‌کند.

2. ساختار نورون مصنوعی (Mathematical Model of an Artificial Neuron)
 اجزای نورون مصنوعی


| جزء                                         | توضیح                                          |
| ------------------------------------------- | ---------------------------------------------- |
| **ورودی‌ها (Inputs: $x_1, x_2, ..., x_n$)** | مقادیر عددی که به نورون وارد می‌شوند           |
| **وزن‌ها (Weights: $w_1, w_2, ..., w_n$)**  | اهمیت هر ورودی در تصمیم‌گیری نورون             |
| **بایاس (Bias: $b$)**                       | مقدار عددی ثابت برای انعطاف‌پذیری بیشتر در مدل |
| **تابع جمع (Weighted Sum)**                 | مجموع وزن‌دار ورودی‌ها به‌علاوه بایاس          |
| **تابع فعال‌سازی (Activation Function)**    | تبدیل خروجی خطی به غیرخطی                      |
| **خروجی (Output: $y$)**                     | نتیجه نهایی نورون پس از عبور از تابع فعال‌سازی |






فرمول ریاضی نورون مصنوعی



   $$z = \sum_{i=1}^{n} w_i x_i + b$$
   
   $$y = f(z)$$
   
   توضیحات:
   

  ورودی‌ها   
  ---
  
   $$- \( x_i \) -$$
  
  وزن هر ورودی  
  
   $$- \( w_i \) -$$
  
   بایاس (bias)  
  
   
   $$- \( b \) -$$
   
   تابع فعال‌سازی (Activation Function)  
   
   $$- \( f(z) \) -$$
   
   خروجی نورون  
   
   $$- \( y \) -$$

---
نمونه‌ای از توابع فعال‌سازی (Activation Functions)

| تابع       | فرمول                                      | دامنه خروجی | مزایا                            |
| ---------- | ------------------------------------------ | ----------- | -------------------------------- |
| Sigmoid    | $f(z) = \frac{1}{1 + e^{-z}}$              | (0, 1)      | نرم‌سازی برای خروجی بین صفر و یک |
| Tanh       | $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1)     | مرکزیت حول صفر                   |
| ReLU       | $f(z) = \max(0, z)$                        | \[0, ∞)     | سریع و مؤثر در شبکه‌های عمیق     |
| Leaky ReLU | $f(z) = \max(0.01z, z)$                    | \~          | حل مشکل مرگ نورون در ReLU        |

    
پیاده‌سازی ساده نورون با NumPy


    import numpy as np
    
    x = np.array([0.5, 0.3, 0.2])
    w = np.array([0.4, 0.7, 0.6])
    b = 0.1
    
    def relu(z):
        return np.maximum(0, z)
    
    z = np.dot(w, x) + b
    y = relu(z)
    
    print("Output:", y)

output
            
            Output --> 0.63



پیاده‌سازی نورون با Keras
        
        import tensorflow as tf
        from tensorflow.keras.layers import Dense
        from tensorflow.keras.models import Sequential
        
        model = Sequential([
            Dense(1, input_shape=(3,), activation='relu')
        ])
        
        model.summary()


output

     output -->
     Model: "sequential"
     
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
    ┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
    │ dense (Dense)                        │ (None, 1)                   │               4 │
    └──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

     Total params: 4 (16.00 B)
     Trainable params: 4 (16.00 B)
     Non-trainable params: 0 (0.00 B)




