3. ุชูุงุจุน ูุนุงูโุณุงุฒ (Activation Functions)


 ฺุฑุง ุจู ุชุงุจุน ูุนุงูโุณุงุฒ ูุงุฒ ุฏุงุฑูุ

   Without activation functions, a neural network becomes a simple linear model, unable to capture complex non-linear patterns.
   
  ุจุฏูู ุชุงุจุน ูุนุงูโุณุงุฒุ ูุฏู ููุท ฺฉ ุชุฑฺฉุจ ุฎุท ุงุฒ ูุฑูุฏโูุง ุฎูุงูุฏ ุจูุฏ ู ุชูุงูุง ุงุฏฺฏุฑ ุฑูุงุจุท ูพฺุฏู ุฑุง ูุฎูุงูุฏ ุฏุงุดุช.

     
 ููุด ุชุงุจุน ูุนุงูโุณุงุฒ:

  ุงุถุงููโฺฉุฑุฏู ุบุฑุฎุท ุจูุฏู ุจู ูุฏู

   ฺฉูุชุฑู ุนุจูุฑ ุณฺฏูุงูโูุง (ูุซู ููุชุฑ)

   ฺฉูฺฉ ุจู ููฺฏุฑุง ุณุฑุนโุชุฑ ุฏุฑ ุขููุฒุด

  ุงููุงุน ุชูุงุจุน ูุนุงูโุณุงุฒ ุฑุงุฌ:

1. Step Function (ุชุงุจุน ูพููโุง)

ูุฏู ู ุบุฑูุงุจู ูุดุชู โ ุจุฑุง ูุฏูโุณุงุฒ ููุฑููโูุง ุจุงูุฑ

$$f(z) =\begin{cases}1 & \text{if } z \geq 0 \\\\0 & \text{if } z < 0\end{cases}$$

โ ุณุงุฏู ู ูุงุจู ููู

โ ูุงููุงุณุจ ุจุฑุง ุงุฏฺฏุฑ ุนูู (ฺฏุฑุงุฏุงู ุตูุฑ)

2. Sigmoid Function (ุณฺฏููุฏ)

$$f(z) = \frac{1}{1 + e^{-z}}$$

๐ ุฎุฑูุฌ ุจู (0, 1)

๐ง ููุงุณุจ ุจุฑุง ูุณุงุฆู ุจุงูุฑ

โ ุฎุฑูุฌ ุงุญุชูุงูโฺฏููู

โ ูุดฺฉู Vanishing Gradient ุฏุฑ ุดุจฺฉูโูุง ุนูู

3. Tanh Function (ุชุงูฺุงูุช ููพุฑุจููฺฉ)

$$f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

๐ ุฎุฑูุฌ ุจู (-1, 1)

โ ูุฑฺฉุฒุช ุญูู ุตูุฑ โ ุขููุฒุด ุจูุชุฑ ูุณุจุช ุจู Sigmoid

โ ูููุฒ ูุดฺฉู ฺฏุฑุงุฏุงู ฺฉูฺฺฉ ุฏุงุฑุฏ ุฏุฑ ูุงูโูุง ุนูู

4. ReLU (Rectified Linear Unit)

$$f(z) =\begin{cases}z & \text{if } z \geq 0 \\\\0 & \text{if } z < 0\end{cases}$$

๐ ูุนุฑููโุชุฑู ู ูพุฑฺฉุงุฑุจุฑุฏุชุฑู ุชุงุจุน ูุนุงูโุณุงุฒ

โ ุณุงุฏฺฏุ ูุดุชูโูพุฐุฑุ ุนููฺฉุฑุฏ ุนุงู

โ ูุดฺฉู "ูุฑฺฏ ููุฑูู" (Dead Neurons) โ ุงฺฏุฑ ูุฑูุฏ ููุดู ููู ุจุงุดุฏุ ููุฑูู ุฎุงููุด ูโูุงูุฏ

5. Leaky ReLU

$$f(z) =\begin{cases}z & \text{if } z \geq 0 \\\\\alpha z & \text{if } z < 0 \quad (\alpha \approx 0.01)\end{cases}$$

โ ุญู ูุดฺฉู ูุฑฺฏ ููุฑูู ุฏุฑ ReLU

โ ุงุฌุงุฒู ุนุจูุฑ ุงูุฏฺฉ ุงุฒ ุจุฎุด ููู

6. Softmax

$$f(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$


๐ ุชุจุฏู ุฎุฑูุฌโูุง ุจู ุงุญุชูุงู ุจุฑุง ุทุจููโุจูุฏ ฺูุฏโฺฉูุงุณู

โ ูุฌููุน ุฎุฑูุฌโูุง = 1

โ ููุงุณุจ ุจุฑุง ูุงูโ ุฎุฑูุฌ ุดุจฺฉูโูุง ุฏุณุชูโุจูุฏ

๐ ุฌุฏูู ููุงุณูโุง:


| ุชุงุจุน       | ุฏุงููู ุฎุฑูุฌ  | ูุดุชูโูพุฐุฑ | ฺฉุงุฑุจุฑุฏ                |
| ---------- | ------------ | ---------- | --------------------- |
| Step       | 0 ุง 1       | โ          | ูุฏูโูุง ุณุงุฏู ุง ูุฏู |
| Sigmoid    | (0, 1)       | โ          | ุทุจููโุจูุฏ ุจุงูุฑ      |
| Tanh       | (-1, 1)      | โ          | ูุงูโูุงูโูุง RNN     |
| ReLU       | \[0, โ)      | โ          | CNNุ ุดุจฺฉูโูุง ุนูู    |
| Leaky ReLU | (-โ, โ)      | โ          | ุฌุงฺฏุฒู ReLU          |
| Softmax    | \[0, 1], ฮฃ=1 | โ          | ุทุจููโุจูุฏ ฺูุฏฺฉูุงุณู    |


