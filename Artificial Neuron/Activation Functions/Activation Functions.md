3. توابع فعال‌سازی (Activation Functions)


 چرا به تابع فعال‌سازی نیاز داریم؟

   Without activation functions, a neural network becomes a simple linear model, unable to capture complex non-linear patterns.
   
  بدون تابع فعال‌سازی، مدل فقط یک ترکیب خطی از ورودی‌ها خواهد بود و توانایی یادگیری روابط پیچیده را نخواهد داشت.

     
 نقش تابع فعال‌سازی:

  اضافه‌کردن غیرخطی بودن به مدل

   کنترل عبور سیگنال‌ها (مثل فیلتر)

   کمک به همگرایی سریع‌تر در آموزش

  انواع توابع فعال‌سازی رایج:

1. Step Function (تابع پله‌ای)

قدیمی و غیرقابل مشتق — برای مدل‌سازی نورون‌های باینری

$$f(z) =\begin{cases}1 & \text{if } z \geq 0 \\\\0 & \text{if } z < 0\end{cases}$$

✅ ساده و قابل فهم

❌ نامناسب برای یادگیری عمیق (گرادیان صفر)

2. Sigmoid Function (سیگموید)

$$f(z) = \frac{1}{1 + e^{-z}}$$

📈 خروجی بین (0, 1)

🧠 مناسب برای مسائل باینری

✅ خروجی احتمال‌گونه

❌ مشکل Vanishing Gradient در شبکه‌های عمیق

3. Tanh Function (تانژانت هیپربولیک)

$$f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$

📈 خروجی بین (-1, 1)

✅ مرکزیت حول صفر → آموزش بهتر نسبت به Sigmoid

❌ هنوز مشکل گرادیان کوچک دارد در لایه‌های عمیق

4. ReLU (Rectified Linear Unit)

$$f(z) =\begin{cases}z & \text{if } z \geq 0 \\\\0 & \text{if } z < 0\end{cases}$$

📈 معروف‌ترین و پرکاربردترین تابع فعال‌سازی

✅ سادگی، مشتق‌پذیری، عملکرد عالی

❌ مشکل "مرگ نورون" (Dead Neurons) – اگر ورودی همیشه منفی باشد، نورون خاموش می‌ماند

5. Leaky ReLU

$$f(z) =\begin{cases}z & \text{if } z \geq 0 \\\\\alpha z & \text{if } z < 0 \quad (\alpha \approx 0.01)\end{cases}$$

✅ حل مشکل مرگ نورون در ReLU

✅ اجازه عبور اندک از بخش منفی

6. Softmax

$$f(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$$


📈 تبدیل خروجی‌ها به احتمال برای طبقه‌بندی چند‌کلاسه

✅ مجموع خروجی‌ها = 1

✅ مناسب برای لایه‌ی خروجی شبکه‌های دسته‌بندی

📊 جدول مقایسه‌ای:


| تابع       | دامنه خروجی  | مشتق‌پذیری | کاربرد                |
| ---------- | ------------ | ---------- | --------------------- |
| Step       | 0 یا 1       | ❌          | مدل‌های ساده یا قدیمی |
| Sigmoid    | (0, 1)       | ✅          | طبقه‌بندی باینری      |
| Tanh       | (-1, 1)      | ✅          | میان‌لایه‌های RNN     |
| ReLU       | \[0, ∞)      | ✅          | CNN، شبکه‌های عمیق    |
| Leaky ReLU | (-∞, ∞)      | ✅          | جایگزین ReLU          |
| Softmax    | \[0, 1], Σ=1 | ✅          | طبقه‌بندی چندکلاسه    |


