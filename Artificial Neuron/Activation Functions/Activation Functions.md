3. توابع فعال‌سازی (Activation Functions)


 چرا به تابع فعال‌سازی نیاز داریم؟

   Without activation functions, a neural network becomes a simple linear model, unable to capture complex non-linear patterns.
   
  بدون تابع فعال‌سازی، مدل فقط یک ترکیب خطی از ورودی‌ها خواهد بود و توانایی یادگیری روابط پیچیده را نخواهد داشت.

     
 نقش تابع فعال‌سازی:

  اضافه‌کردن غیرخطی بودن به مدل

   کنترل عبور سیگنال‌ها (مثل فیلتر)

   کمک به همگرایی سریع‌تر در آموزش

  انواع توابع فعال‌سازی رایج:

1. Step Function (تابع پله‌ای)

قدیمی و غیرقابل مشتق — برای مدل‌سازی نورون‌های باینری
$$f(z)={1if z≥00if z<0$$
$$f(z)={10​if z≥0if z<0​$$

✅ ساده و قابل فهم
❌ نامناسب برای یادگیری عمیق (گرادیان صفر)
2. Sigmoid Function (سیگموید)

$$f(z)=11+e−z$$
$$f(z)=1+e−z1​$$

📈 خروجی بین (0, 1)
🧠 مناسب برای مسائل باینری

✅ خروجی احتمال‌گونه
❌ مشکل Vanishing Gradient در شبکه‌های عمیق
3. Tanh Function (تانژانت هیپربولیک)
$$f(z)=ez−e−zez+e−z$$
$$f(z)=ez+e−zez−e−z​$$

📈 خروجی بین (-1, 1)

✅ مرکزیت حول صفر → آموزش بهتر نسبت به Sigmoid
❌ هنوز مشکل گرادیان کوچک دارد در لایه‌های عمیق
4. ReLU (Rectified Linear Unit)
$$f(z)=max⁡(0,z)$$
$$f(z)=max(0,z)$$

📈 معروف‌ترین و پرکاربردترین تابع فعال‌سازی

✅ سادگی، مشتق‌پذیری، عملکرد عالی
❌ مشکل "مرگ نورون" (Dead Neurons) – اگر ورودی همیشه منفی باشد، نورون خاموش می‌ماند
5. Leaky ReLU
$$f(z)={zif z≥0αzif z<0(α≈0.01)$$
$$f(z)={zαz​if z≥0if z<0(α≈0.01)​$$

✅ حل مشکل مرگ نورون در ReLU
✅ اجازه عبور اندک از بخش منفی
6. Softmax

$$f(zi)=ezi∑jezj$$
$$f(zi​)=∑j​ezj​ezi​​$$

📈 تبدیل خروجی‌ها به احتمال برای طبقه‌بندی چند‌کلاسه

✅ مجموع خروجی‌ها = 1
✅ مناسب برای لایه‌ی خروجی شبکه‌های دسته‌بندی
📊 جدول مقایسه‌ای:

| تابع       | دامنه خروجی  | مشتق‌پذیری | کاربرد                |
| ---------- | ------------ | ---------- | --------------------- |
| Step       | 0 یا 1       | ❌          | مدل‌های ساده یا قدیمی |
| Sigmoid    | (0, 1)       | ✅          | طبقه‌بندی باینری      |
| Tanh       | (-1, 1)      | ✅          | میان‌لایه‌های RNN     |
| ReLU       | \[0, ∞)      | ✅          | CNN، شبکه‌های عمیق    |
| Leaky ReLU | (-∞, ∞)      | ✅          | جایگزین ReLU          |
| Softmax    | \[0, 1], Σ=1 | ✅          | طبقه‌بندی چندکلاسه    |


