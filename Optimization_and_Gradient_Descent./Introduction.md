Introduction to Optimization

مقدمه‌ای بر بهینه‌سازی در شبکه‌های عصبی 

هدفِ یادگیری، کمینه‌کردن تابع هزینه/خطا `J(θ)` است؛ که `θ` مجموعه‌ی وزن‌ها و بایاس‌هاست.  
برای یک مسئله‌ی نظارت‌شده:

$$
\frac{1}{N} \sum_{i=1}^{N} L\big(f(x_i; \theta), \; y_i\big)
$$

چالش اصلی: منظرِ خطا (Loss Landscape) غیرکوژ (non‑convex) است → مینیمم‌های محلی، نواحی تخت (plateaus)، زین‌نقطه‌ها (saddles) و درّه‌های باریک (ravines).

ابزار اصلی: گرادیان `∇θJ(θ)` که جهتِ بیشترین افزایش را نشان می‌دهد؛  
پس جهتِ نزول `-∇` است.  

سه پارامتر ذهنی مهم:

- نرخ یادگیری `η`: قدمِ حرکت روی سطح خطا؛  
 خیلی بزرگ → جهش و ناپایداری  
  خیلی کوچک → همگرایی کند  

- مقیاس‌گذاری ویژگی‌ها (Normalization / Standardization): بهبود هندسه‌ی منظر و سرعت همگرایی.  

- تصادفی‌بودن (Batching / Noise): نویز گرادیان گاهی کمک می‌کند از زین‌نقطه‌ها فرار کنیم.
