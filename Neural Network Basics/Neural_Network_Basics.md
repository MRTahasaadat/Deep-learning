What is a Neural Network ?

 Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú†ÛŒØ³ØªØŸ



Neural networks are computational models inspired by the structure of the human brain.

They consist of layers of artificial neurons that process input data to learn patterns and make decisions or predictions.

Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¨Ø§ Ø§Ù„Ù‡Ø§Ù… Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ù…ØºØ² Ø§Ù†Ø³Ø§Ù† Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.

Ø§ÛŒÙ† Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØµÙ†ÙˆØ¹ÛŒ ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ú©Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯ Ùˆ ØªØµÙ…ÛŒÙ… ÛŒØ§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù†Ø¯.

1. Components of a Neural Network

1. Ø§Ø¬Ø²Ø§ÛŒ Ø§ØµÙ„ÛŒ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ

 Layers

    Input Layer: where the data enters the network

    Hidden Layer(s): intermediate layers where features are extracted

    Output Layer: final layer that gives prediction or decision

 Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§


    Ù„Ø§ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒ: Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ ÙˆØ§Ø±Ø¯ Ø´Ø¨Ú©Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯

    Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†: Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…ÛŒØ§Ù†ÛŒ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯

    Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ: Ù„Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÛŒØ§ ØªØµÙ…ÛŒÙ… Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

Neurons (Units)

 Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ (ÛŒÙˆÙ†ÛŒØªâ€ŒÙ‡Ø§)

Each layer consists of neurons (also called units or nodes).

Each neuron receives inputs, multiplies them by weights, adds a bias, and applies an activation function.

Ù‡Ø± Ù„Ø§ÛŒÙ‡ Ø§Ø² Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ (Ú©Ù‡ Ú¯Ø§Ù‡ÛŒ Ø¨Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ Ú¯Ø±Ù‡ ÛŒØ§ ÛŒÙˆÙ†ÛŒØª Ù‡Ù… Ú¯ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯) ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª.

Ù‡Ø± Ù†ÙˆØ±ÙˆÙ†ØŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¶Ø±Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¨Ø§ÛŒØ§Ø³ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ù¾Ø³ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Connections

 Ø§ØªØµØ§Ù„Ø§Øª


Neurons in one layer are fully connected to the neurons in the next layer.
Each connection has a weight that determines the influence of the input.

Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ù„Ø§ÛŒÙ‡ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ø¨Ø¹Ø¯ÛŒ ÙˆØµÙ„ Ù‡Ø³ØªÙ†Ø¯.
Ù‡Ø± Ø§ØªØµØ§Ù„ Ø¯Ø§Ø±Ø§ÛŒ ÛŒÚ© ÙˆØ²Ù† (Weight) Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒØ²Ø§Ù† ØªØ£Ø«ÛŒØ± ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Visual Representation

        Input Layer      Hidden Layer     Output Layer
      [ x1 ] â€”â€”â”       [ h1 ]           [ y1 ]
      [ x2 ] â€”â€”â”¼â€”â€”â†’    [ h2 ] â€”â€”â†’       [ y2 ]
      [ x3 ] â€”â€”â”˜       [ h3 ]


Simple Math Behind a Layer

Ø±ÛŒØ§Ø¶ÛŒØ§Øª Ø³Ø§Ø¯Ù‡ Ù¾Ø´Øª ÛŒÚ© Ù„Ø§ÛŒÙ‡

For one hidden layer with 3 inputs and 1 neuron:

Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù„Ø§ÛŒÙ‡ Ù¾Ù†Ù‡Ø§Ù† Ø¨Ø§ Û³ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Û± Ù†ÙˆØ±ÙˆÙ†:
    
    z = w1 * x1 + w2 * x2 + w3 * x3 + b
    a = activation(z)


$wiâ€‹$: ÙˆØ²Ù†â€ŒÙ‡Ø§

$xixi$â€‹: ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§

$bb$: Ø¨Ø§ÛŒØ§Ø³

$aa$: Ø®Ø±ÙˆØ¬ÛŒ Ù†ÙˆØ±ÙˆÙ† Ù¾Ø³ Ø§Ø² ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
        
        | Ù…ÙÙ‡ÙˆÙ…              | Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ                  | ÙØ§Ø±Ø³ÛŒ                   |
        | ------------------ | ------------------------ | ----------------------- |
        | Data Entry         | Input Layer              | Ù„Ø§ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒ              |
        | Feature Extraction | Hidden Layer(s)          | Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù†          |
        | Prediction         | Output Layer             | Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ              |
        | Math Operation     | Neuron Computation       | Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø± Ù†ÙˆØ±ÙˆÙ†         |
        | Learning           | Adjusting weights/biases | ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§ Ùˆ Ø¨Ø§ÛŒØ§Ø³â€ŒÙ‡Ø§ |


Forward Propagation

Ù¾Ø±Ø§Ù¾Ú¯ÛŒØ´Ù† Ø±Ùˆ Ø¨Ù‡ Ø¬Ù„Ùˆ (Ø§Ù†ØªÙ‚Ø§Ù„ Ø±Ùˆ Ø¨Ù‡ Ø¬Ù„ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ø´Ø¨Ú©Ù‡)

What is Forward Propagation?

Ù¾Ø±Ø§Ù¾Ú¯ÛŒØ´Ù† Ø±Ùˆ Ø¨Ù‡ Ø¬Ù„Ùˆ Ú†ÛŒØ³ØªØŸ

Forward propagation is the process of passing input data through the network, layer by layer, to compute the final output (prediction).
At each neuron, the inputs are combined, transformed, and passed forward to the next layer.

Ù¾Ø±Ø§Ù¾Ú¯ÛŒØ´Ù† Ø±Ùˆ Ø¨Ù‡ Ø¬Ù„Ùˆ Ø¨Ù‡ ÙØ±Ø¢ÛŒÙ†Ø¯ÛŒ Ú¯ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ø¢Ù† Ø¯Ø§Ø¯Ù‡â€ŒÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¹Ø¨ÙˆØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ) ØªÙˆÙ„ÛŒØ¯ Ø´ÙˆØ¯.
Ø¯Ø± Ù‡Ø± Ù†ÙˆØ±ÙˆÙ†ØŒ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ ØªØ±Ú©ÛŒØ¨ØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¨Ù‡ Ù„Ø§ÛŒÙ‡ Ø¨Ø¹Ø¯ÛŒ Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.

Step-by-Step Process

ÙØ±Ø¢ÛŒÙ†Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡

Step 1: Weighted Sum

Ù…Ø±Ø­Ù„Ù‡ Û±: Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ²Ù†ÛŒ

Each neuron calculates a weighted sum of its inputs plus a bias.

Ù‡Ø± Ù†ÙˆØ±ÙˆÙ† Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ²Ù†ÛŒ Ø§Ø² ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ÛŒÚ© Ø¨Ø§ÛŒØ§Ø³ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

      z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b


Step 2: Activation

 Ù…Ø±Ø­Ù„Ù‡ Û²: ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ

Apply an activation function to add non-linearity:

Ø¨Ø±Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØºÛŒØ±Ø®Ø·ÛŒ Ø¨ÙˆØ¯Ù† Ø¨Ù‡ Ù…Ø¯Ù„ØŒ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯:

    a = activation(z)

Example with Numbers
    
    Inputs (x):  [1.0, 2.0, 3.0]
    Weights (w): [0.5, -1.0, 0.7]
    Bias (b): 0.1

Calculation of z

    z = (0.5*1.0) + (-1.0*2.0) + (0.7*3.0) + 0.1
    = 0.5 - 2.0 + 2.1 + 0.1 = 0.7
    
If the activation function is ReLU

    a = max(0, 0.7) = 0.7


Python Example with NumPy

    import numpy as np
    
    X = np.array([1.0, 2.0, 3.0])
    W = np.array([0.5, -1.0, 0.7])
    b = 0.1
    
    z = np.dot(W, X) + b
  
    a = np.maximum(0, z) --> # ReLU activation 
    
    print("Output:", a)

  output --> 

    Output: 0.7

Repeat for All Neurons and Layers
    
 ØªÚ©Ø±Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ùˆ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§


 Forward propagation is repeated for each neuron in each layer, passing the results forward to the next layer.

 
Ù¾Ø±Ø§Ù¾Ú¯ÛŒØ´Ù† Ø±Ùˆ Ø¨Ù‡ Ø¬Ù„Ùˆ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø¯Ø± ØªÙ…Ø§Ù… Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ ØªÚ©Ø±Ø§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ùˆ Ù†ØªØ§ÛŒØ¬ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ù…Ù†ØªÙ‚Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.

Summary 
      
      | Ú¯Ø§Ù… | Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ      | ÙØ§Ø±Ø³ÛŒ                |
      | --- | ------------ | -------------------- |
      | 1   | Weighted Sum | Ù…Ø¬Ù…ÙˆØ¹ ÙˆØ²Ù†ÛŒ           |
      | 2   | Add Bias     | Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ø§ÛŒØ§Ø³         |
      | 3   | Activation   | Ø§Ø¹Ù…Ø§Ù„ ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ |
      | 4   | Forward Pass | Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ù„Ø§ÛŒÙ‡ Ø¨Ø¹Ø¯ÛŒ  |

Loss Function

ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ (ØªØ§Ø¨Ø¹ Ù‡Ø²ÛŒÙ†Ù‡)

What is a Loss Function?

 ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ Ú†ÛŒØ³ØªØŸ
 

A loss function is a mathematical way to measure how wrong the model's predictions are compared to the actual (true) values.
It returns a single number that represents the error.


ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ ÛŒÚ© Ø±ÙˆØ´ Ø±ÛŒØ§Ø¶ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù…ÛŒØ²Ø§Ù† Ø§Ø´ØªØ¨Ø§Ù‡ Ù…Ø¯Ù„ Ø§Ø³ØªØ› ÛŒØ¹Ù†ÛŒ ØªÙØ§ÙˆØª Ø¨ÛŒÙ† Ø®Ø±ÙˆØ¬ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡ ØªÙˆØ³Ø· Ù…Ø¯Ù„ Ùˆ Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ.
Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÛŒÚ© Ø¹Ø¯Ø¯ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ú©Ù‡ Ù…ÛŒØ²Ø§Ù† Ø®Ø·Ø§ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

 Why Is It Important?

ğŸ¯ Ú†Ø±Ø§ Ù…Ù‡Ù… Ø§Ø³ØªØŸ
    
It guides the learning process: the model tries to minimize the loss.
A smaller loss means the model is doing better.


ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ÙØ±Ø§ÛŒÙ†Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø³Øª: Ù…Ø¯Ù„ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø§ÛŒÙ† Ø®Ø·Ø§ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡Ø¯.
Ù‡Ø±Ú†Ù‡ Ø®Ø·Ø§ Ú©Ù…ØªØ± Ø¨Ø§Ø´Ø¯ØŒ ÛŒØ¹Ù†ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ØªØ± ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª


General Form

      Loss = loss_function(y_true, y_pred)


Types of Loss Functions

Ø§Ù†ÙˆØ§Ø¹ ØªØ§Ø¨Ø¹ Ø®Ø·Ø§

1. Mean Squared Error (MSE)

 Common for regression tasks

Formula:

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^n \left( y_{\text{true}, i} - y_{\text{pred}, i} \right)^2$$

Explanation: Measures average squared difference between predicted and actual values. Penalizes large errors more heavily.

ØªÙˆØ¶ÛŒØ­: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø®ØªÙ„Ø§Ù Ù…Ø±Ø¨Ø¹ Ø´Ø¯Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¨ÛŒØ´ØªØ± Ø¬Ø±ÛŒÙ…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Python example:

    import numpy as np
    
    y_true = np.array([3.0, 2.5, 1.0])
    y_pred = np.array([2.5, 2.0, 0.5])
    
    mse = np.mean((y_true - y_pred)**2)
    print("MSE:", mse)

2. Mean Absolute Error (MAE)

Use case: Regression

Ú©Ø§Ø±Ø¨Ø±Ø¯: Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†

Formula:
ÙØ±Ù…ÙˆÙ„:

$$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|$$

Explanation: Measures average absolute difference between predicted and true values. More robust to outliers than MSE.

ØªÙˆØ¶ÛŒØ­: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø®ØªÙ„Ø§Ù Ù‚Ø¯Ø±Ù…Ø·Ù„Ù‚ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ù…Ù‚Ø§ÙˆÙ…â€ŒØªØ± Ø§Ø³Øª.

Python example:
    
    import numpy as np
    
    y_true = np.array([3.0, 2.5, 1.0])
    y_pred = np.array([2.5, 2.0, 0.5])
    
    mae = np.mean(np.abs(y_true - y_pred))
    print("MAE:", mae)


3. Binary Cross-Entropy (Log Loss)

Use case: Binary classification

Ú©Ø§Ø±Ø¨Ø±Ø¯: Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ

$$\text{BCE} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]$$


Explanation: Measures difference between true labels and predicted probabilities. Penalizes confident wrong predictions heavily

ØªÙˆØ¶ÛŒØ­: ØªÙØ§ÙˆØª Ø¨ÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚Ø·Ø¹ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ø±Ø§ Ø´Ø¯ÛŒØ¯ØªØ± Ø¬Ø±ÛŒÙ…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.


Python example:

    imoprt numpy as np
    
    y_true = np.array([1, 0, 1])
    y_pred = np.array([0.9, 0.2, 0.8])
    
    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    print("Binary Cross-Entropy:", bce)



4. Categorical Cross-Entropy

Use case: Multi-class classification

Ú©Ø§Ø±Ø¨Ø±Ø¯: Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡

Formula:

ÙØ±Ù…ÙˆÙ„:

$$\text{CCE} = - \sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij} \log(p_{ij})$$

where kk is number of classes, yijyijâ€‹ is true label (one-hot encoded), and pijpijâ€‹ is predicted probability.

Explanation: Measures difference between one-hot true labels and predicted class probabilities from softmax output.

Ú©Ù‡ kk ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø§Ø³ØªØŒ yijyijâ€‹ Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª ÙˆØ§Ù†-Ù‡Ø§ØªØŒ Ùˆ pijpijâ€‹ Ø§Ø­ØªÙ…Ø§Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.

ØªÙˆØ¶ÛŒØ­: ØªÙØ§ÙˆØª Ø¨ÛŒÙ† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù†-Ù‡Ø§Øª ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ softmax Ø±Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.


Python example (TensorFlow):

      import tensorflow as tf
      
      loss_fn = tf.keras.losses.CategoricalCrossentropy()
      
      y_true = [[0, 0, 1]]
      y_pred = [[0.1, 0.2, 0.7]]
      
      loss = loss_fn(y_true, y_pred).numpy()
      print("Categorical Cross-Entropy:", loss)


5. Hinge Loss

Use case: SVM classification

Ú©Ø§Ø±Ø¨Ø±Ø¯: Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ SVM

Formula:

ÙØ±Ù…ÙˆÙ„:

$$\text{Hinge} = \sum_{i=1}^{n} \max\left(0, 1 - y_i \hat{y}_i\right)$$

$$where yiâˆˆ{âˆ’1,1}yiâ€‹âˆˆ{âˆ’1,1}.$$

Explanation: Penalizes predictions that are not confidently correct.

No simple Python example here, but often used in SVM libraries

Ú©Ù‡ yiyiâ€‹ Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¢Ù† -1 ÛŒØ§ +1 Ø§Ø³Øª.

ØªÙˆØ¶ÛŒØ­: Ø¬Ø±ÛŒÙ…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ú©Ø§ÙÛŒ Ø¯Ø±Ø³Øª Ù†ÛŒØ³ØªÙ†Ø¯.
